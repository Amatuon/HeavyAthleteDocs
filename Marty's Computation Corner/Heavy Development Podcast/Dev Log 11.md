Hi there this is Michael from HeavyAthlete.com I decided to rerecord this episode after the site failure yesterday. Before it was about the out of memory issue we hit last week. Now that I have a second different failure I decided to go a bit more general on server management problems. So consider this an official follow up to the first episode which shouldn't have been the first episode.

Okay so lets pretend you are a small startup. You've been renting your compute for $5 a month. If you are bleh... server less pretend like you're successful and don't want to pay infinite money a month. Both of these scenarios have the same solution self hosting. It sounds great one time fees to buy new hardware to scale vertically. I mean that alone is huge you get to save up money from month to month and reinvest it in your business instead of paying a landlord I mean a cloud platform. If your rebuttal is that the cost isn't that important then thanks for listening but you aren't the target audience as you lack the entrepreneurial spirit. Come back next week maybe I'll be talking about something more to your liking. Okay are they all gone. Is it only startups that stand a chance of success left. Great. Then lets talk about the draw backs.

As a small startup you under stand the fact that buck stops here. There is no one else to blame, there are no excuses you either deliver a high quality product or service or you fail. So all of these issues should sound familiar to you. 

First off you can run out of resources unexpectedly. This one isn't unique to self hosted machines. But his one hit me the other week and caused the site the crash. I hit an OOM or "out of memory" error. While your CPU is basically a boundless resource even on a low power machine. Your memory is always finite. First lets talk about why CPU isn't really a problem then I'll move on to memory. Even if you max out all your CPU cores nothing crashes. You might be delayed in some responses you might even miss some requests. But if the worst thing that happens is buffering or needing to refresh the page then your doing alright. Memory is the more insidious killer. Once you are out of memory that's it everything freezes your system is unresponsive and you can't fix it. Programs fundamentally live in memory. The next command to execute lives there too. If you run out of space then you can't even load a command to free up space. So what can you do? I drove to my server and hit it with the most powerful IT wizardry I know. I turned it off and on again. I know that's pretty advanced stuff but its okay I know you can do it too. This fixes the problem temporarily but the underlying cause of the problem remains. There are several things to do if you are aren't in a memory safe language patch your memory leak. Don't have a memory leak? That's actually worse because this is going to be your first instinct and you are going to waste a lot of time searching for something that isn't there. This is why I suggest using a memory safe language. I know that won't convince the C grey beards but nothing will. Second thing you can do to fix this is buy more ram. An OOM is your computer telling you that the time for more ram was yesterday. The final thing you can do if your too cheap even to upgrade your hardware is to set up a small secondary computer whose job it is to check your website every x seconds and trigger the hardware reset pin on your main machine. I know what your thinking "isn't that just automatically turning it off and on again?" Yes, yes it is. This incredibly powerful IT spell can actually be run automatically. Now you might be thinking "um ackshullay" there is another solution you could refactor the product and improve memory usage everywhere. ... Ha HaHa HAHAHAHHAHAHAHAHAA! Oh that's a good one. But you forgot something. In this thought experiment you're a small startup. You're already the best programmer you have and you are the one who made the problems in the first place. Besides your customer's feature requests are as endless as they are unreasonable. It is almost always a better use of your time in this scenario to work on something else then to chase a nebulous x% improvement across the board.

So now that I've talked about the dreaded OOM let me tell you about the one that took down Heavy Athlete. I have a script that needs to run once a day to check other sites for data and import it if anything has changed. My rented cloud computer had a finite data transfer limit. Hitting that would have been a hell of an OOM. So I had my dev pc set up to run the script daily, process the fetched data and then send the results to the server. So why did I change anything? Simple I didn't want the script running on my dev machine. I don't like that an inherently unstable environment is responsible for a critical workflow. So I moved the script onto then server itself. This was okay because my own internet is rate limited but not volume limited. So there was no longer a need to handle the high data transfer job elsewhere now. So what was it? A critical bug? An infinite loop? Did I fork bomb myself? Nope I confused the hell out of git. I'm going to have to wave my hands a little as I explain this as I haven't found exactly what caused the issue and therefore how to fix it. Basically the script downloads some data in one format and saves it to the disk in an optimized format. Occasionally we have to do a full refresh to look for changes and the optimized compressed data format uses over 500 MB of disk space so trying to hold all the optimized memory in ram is just dumb. Saving these files isn't the issue but it does kick start the problem. Git notices the file changes and new files and begins its magic. The script also sends the optimized payload to the server. The server verifies the identity of the sender and then saves the data in a different optimized format for later use. Now that we are on the same machine it overwrites the the files from before and git notices the changes and starts its magic. Apparently I have the timing just right to confuse the hell out of git. Watching this event using htop gits memory usage balloons and consumes everything. I've been looking into it and trying to figure out the core problem so that I can correct it and help support such an amazing open source tool as git but I haven't found the root problem yet. So what was the solution? I moved the script to a different machine again. Now git isn't confused and the site runs normally. 

With story time over lets talk about the other issue that brought the site down. We had a hell of a thunderstorm. There is an old saying, "If you want to see God smile tell him your plans." I live out in the woods so when the power goes out it I am the bottom of the power companies priority que. Also my internet is delivered via microwaves which has the interesting side effect that in torrential rain my internet literally becomes steam. With that in mind there was no way I was keeping my server at home. My brother owns his own gym. He has far more stable internet and power than I do at home. So the server lives in his gym. Many of the athletes at his gym actually compete in the highland games and know the server sitting in the corner is what runs Heavy Athlete. I don't know if they think that's as cool as I think it is. Anyway, While at a game yesterday a storm rolled through at the gym. Power went out briefly. This wouldn't have been a problem except that the server I am running on is a cluster of raspberry pis. These things don't have as much capacitance as a normal consumer electronic. When the power browns out briefly most things keep working because capacitors are magic. Not really but explaining capacitors will have to wait for another day. Capacitors are while when the power flickers very quickly you don't have to reset the clock on your microwave. Computers are very sensitive to power fluctuations because most things pass through ram which is volatile memory. That means if the power dies the memory is cleared. So I can't even begin to tell you what exactly happened to the state of the computer. But it appears as though the pis did not have the capacitance to ride out the power flickering and got themselves into invalid crash states. I fixed this once more with powerful IT magic. Say it with me "I turned it off and back on again". After that I went home and rode out the storm. The power then flickered again the next morning so when I woke up I had to go and cast the spell one more time. With this weakness identified I have since put the pis on an uninterruptable power supply. I haven't been able to combat test this yet as the rain is gone but I think it will work.

If you have heard these two stories and think man I should just keep using my cloud hosting provider that's fine. But it wasn't the point I was trying to make. There will be errors there will be problems but they can be overcome. I moved off of my cloud provider when I started hitting throughput bottlenecks. Watching the CPU usage graphs I could tell you exactly when highland games athletes got off work. I either had to 10x my monthly cost to run the site or I had to start self hosting. When you look at the actually hardware you rent for the cheep $5 a month plan you will quickly realize that its just a virtual machine suspiciously close to the performance of a single core on a raspberry pi. You can afford to self host, you can figure things out as your go, and you can deliver a great experience to customers while you figure things out. People are quick to rage when computers don't "just work" but in my experience when you calmly explain the struggles they calm down. Most people are not programmers and think that computers are magic. When you admit to them that you are only human and not a wizard they tend to deal with you with a bit more grace.

This has been Michael McDougal from Heavy Athlete. I'll see you next time after I break some more things.